[training]
batch_size = 4
gradient_accumulation_steps = 4
total_round = 6
drop_out = 0.5
num_workers = 2
step1_epochs = 10
step2_epochs = 10
num_protos = 1
device = cuda:4
seed = 100
max_grad_norm = 10
task_length = 8
kl_temp = 2
temp = 0.1
temp_options = [1.5, 2, 2.5]
weight_options = [0.1, 0.2, 0.3]

[Encoder]
bert_path = /home/xurf23/GM_GEN-main/bert-base-uncased
max_length = 256
vocab_size = 30522
marker_size = 4
pattern = entity_marker
encoder_output_size =768


[dropout]
drop_p = 0.1
f_pass = 10
kappa_neg = 0.03
kappa_pos = 0.05


[scheduler]
T_mult = 1
rewarm_epoch_num = 2
# StepLR
decay_rate = 0.9
decay_steps = 800
